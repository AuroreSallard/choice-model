import pickle, os
import requests as reqlib

import subprocess
import time

import pandas as pd
import numpy as np
import cma

# TODO create a nice class instead of a dictionary
def define_parameters():

    reference_path   = "/home/asallard/Documents/Tests/EqasimServer/choice-model/results_aurore/transit/reference_MZ_sample.parquet"
    routing_endpoint = "http://localhost:8054/router/transit"
    output_path      = "/home/asallard/Documents/Tests/EqasimServer/choice-model/results_aurore/transit/calibration.p"
    objective        = "distribution_based"
    jar_path         = "/home/asallard/Euler_project/java/eqasim-java/switzerland/target/switzerland-2.0.0.jar:/home/asallard/Euler_project/java/eqasim-java/server/target/server-2.0.0.jar"

    maximum_batch_size = 4000
    modes_weight       = 1.0
    transfers_weight   = 1.0
    eval_intermodality = True

    seed       = 42
    sigma      = 0.5
    iterations = 4
    pop_size   = 6 # Number of candidates generated by each iteration of the evolutionary optimization algorithm

    parameters = {
        "reference_path": reference_path,
        "routing_endpoint": routing_endpoint,
        "objective": objective,
        "output_path": output_path,
        "maximum_batch_size": maximum_batch_size,
        "modes_weight": modes_weight,
        "transfers_weight": transfers_weight,
        "evaluate_intermodality": eval_intermodality,
        "seed": seed,
        "sigma": sigma,
        "popsize": pop_size,
        "iterations": iterations,
        "jar_path": jar_path
    }

    return parameters


# Read the reference file and process it into requests
def read_reference(parameters):

    df_reference = pd.read_parquet(parameters["reference_path"])

    # Identify modes and maximum transfers
    modes = [c.replace("legs_", "") for c in df_reference.columns if c.startswith("legs_")]
    maximum_transfers = df_reference["transfers"].max()

    parameters["modes"] = modes
    parameters["max_nb_transfers"] = maximum_transfers

    transfer_columns = [col for col in df_reference.columns if "->" in col]
    intramodal_transfer_columns = [col for col in transfer_columns if col.split("->")[0] == col.split("->")[1] ]
    intermodal_transfer_columns = [col for col in transfer_columns if not col in intramodal_transfer_columns]

    df_reference["intramodal_transfers"] = df_reference[intramodal_transfer_columns].sum(axis=1)
    df_reference["intermodal_transfers"] = df_reference[intermodal_transfer_columns].sum(axis=1)

    parameters["max_nb_transfers_intermodal"] = df_reference["intermodal_transfers"].max()
    parameters["max_nb_transfers_intramodal"] = df_reference["intramodal_transfers"].max()

    return parameters, df_reference


def create_requests(df_reference):
    requests = []

    df_reference["request_index"] = np.arange(len(df_reference))

    for _, row in df_reference.iterrows():
        requests.append({
            "request_index": int(row["request_index"]),
            "origin_x": row["origin_x"],
            "origin_y": row["origin_y"],
            "destination_x": row["destination_x"],
            "destination_y": row["destination_y"],
            "departure_time_s": row["departure_time"]
        })

    return requests


def count_transfers(modes):
    intermodal = 0
    intramodal = 0
    for i in range(len(modes) - 1):
        if modes[i] == modes[i + 1]:
            intramodal += 1
        else:
            intermodal += 1
    return intermodal, intramodal


# Prepare querying the routing server
def query_endpoint(parameters, requests, utilities):
    response = reqlib.post(parameters["routing_endpoint"], json = {
        "batch": requests,
        "utilities": utilities
    })

    assert response.status_code == 200

    df_response = { 
        "request_index": [],
        "transfers": [],
        "intermodal_transfers": [],
        "intramodal_transfers": []
    }

    modes_and_subway = parameters["modes"].copy()
    modes_and_subway.append("subway")

    for mode in parameters["modes"]:
        df_response["legs_{}".format(mode)] = []

    for row in response.json():
        df_response["request_index"].append(row["request_index"])
        df_response["transfers"].append(row["transfers"])
        
        for mode in modes_and_subway:
            if mode in parameters["modes"]:
                if mode in row["vehicle_legs_by_mode"]:
                    df_response["legs_{}".format(mode)].append(row["vehicle_legs_by_mode"][mode])
                else:
                    df_response["legs_{}".format(mode)].append(0)
            else:
                if mode in row["vehicle_legs_by_mode"]:
                    df_response["legs_tram"][-1] += row["vehicle_legs_by_mode"][mode]

        transfers_intermodal, transfers_intramodal = count_transfers(row["modes_sequence"])
        df_response["intermodal_transfers"].append(transfers_intermodal)
        df_response["intramodal_transfers"].append(transfers_intramodal)

    df_response = pd.DataFrame(df_response)
    return df_response


# Send the requests to the server
def query_endpoint_batched(parameters, requests, utilities):
    df_response = []
    batch_index = 0

    maximum_batch_size = parameters["maximum_batch_size"]

    while batch_index * maximum_batch_size < len(requests):
        df_response.append(query_endpoint(parameters, 
            requests[batch_index * maximum_batch_size : (batch_index + 1) * maximum_batch_size],
            utilities))
        
        batch_index += 1
    
    return pd.concat(df_response)


# Set up before the server runs
def wait_for_server(url, requests_sample, utilities, timeout=600, interval=10):
    """Wait until the server at `url` responds successfully."""
    start_time = time.time()
    while time.time() - start_time < timeout:
        print("Waiting for server...")
        try:
            response = reqlib.post(parameters["routing_endpoint"], json = {
                    "batch": requests_sample,
                    "utilities": utilities
                })
            if response.status_code == 200:
                print("The server is running!")
                return True
        except reqlib.ConnectionError:
            pass
        time.sleep(interval)
    raise TimeoutError(f"Server at {url} didn't start within {timeout} seconds.")


# Define the optimization objective: observation comparison based
def calculate_objective_observation_based(parameters, df_reference, df_evaluation):
    transfers_weight = parameters["transfers_weight"]
    modes_weight     = parameters["modes_weight"]
    modes            = parameters["modes"]

    df_evaluation = pd.merge(df_reference, df_evaluation, on = "request_index", 
        suffixes = ["_reference", "_evaluation"])
    
    df_evaluation["offset"] = transfers_weight * np.abs(
        df_evaluation["transfers_reference"] - df_evaluation["transfers_evaluation"])

    for mode in modes:
        df_evaluation["offset"] += modes_weight * np.abs(
            df_evaluation["legs_{}_reference".format(mode)] - df_evaluation["legs_{}_evaluation".format(mode)]
        )

    return np.sum(df_evaluation["offset"] * df_evaluation["weight"]) / df_evaluation["weight"].sum()


# Define the optimization objective: distribution comparison based
def calculate_objective_distribution_based(parameters, df_reference, df_evaluation):
    transfers_weight = parameters["transfers_weight"]
    modes_weight     = parameters["modes_weight"]
    modes            = parameters["modes"]
    max_transfers    = parameters["max_nb_transfers"] 

    df_evaluation = pd.merge(df_reference, df_evaluation, on = "request_index", 
        suffixes = ["_reference", "_evaluation"])
    
    reference_mode_distribution = []
    evaluation_mode_distribution = []

    for mode in modes:
        reference_mode_distribution.append((df_evaluation["legs_{}_reference".format(mode)] * df_evaluation["weight"]).sum())
        evaluation_mode_distribution.append((df_evaluation["legs_{}_evaluation".format(mode)] * df_evaluation["weight"]).sum())

    reference_mode_distribution = np.array(reference_mode_distribution) / np.sum(reference_mode_distribution)
    evaluation_mode_distribution = np.array(evaluation_mode_distribution) / np.sum(evaluation_mode_distribution)

    if parameters["evaluate_intermodality"]:

        max_intermodal_transfers = parameters["max_nb_transfers_intermodal"]
        max_intramodal_transfers = parameters["max_nb_transfers_intramodal"]

        reference_inter_transfer_distribution = []
        evaluation_inter_transfer_distribution = []

        for transfers in range(max_intermodal_transfers + 1):
            f_reference = df_evaluation["intermodal_transfers_reference"] == transfers
            reference_inter_transfer_distribution.append(df_evaluation.loc[f_reference, "weight"].sum())

            f_evaluation = df_evaluation["intermodal_transfers_evaluation"] == transfers
            evaluation_inter_transfer_distribution.append(df_evaluation.loc[f_evaluation, "weight"].sum())

        reference_inter_transfer_distribution = np.array(reference_inter_transfer_distribution) / np.sum(reference_inter_transfer_distribution)
        evaluation_inter_transfer_distribution = np.array(evaluation_inter_transfer_distribution) / np.sum(evaluation_inter_transfer_distribution)

        reference_intra_transfer_distribution = []
        evaluation_intra_transfer_distribution = []

        for transfers in range(max_intramodal_transfers + 1):
            f_reference = df_evaluation["intramodal_transfers_reference"] == transfers
            reference_intra_transfer_distribution.append(df_evaluation.loc[f_reference, "weight"].sum())

            f_evaluation = df_evaluation["intramodal_transfers_evaluation"] == transfers
            evaluation_intra_transfer_distribution.append(df_evaluation.loc[f_evaluation, "weight"].sum())

        reference_intra_transfer_distribution = np.array(reference_intra_transfer_distribution) / np.sum(reference_intra_transfer_distribution)
        evaluation_intra_transfer_distribution = np.array(evaluation_intra_transfer_distribution) / np.sum(evaluation_intra_transfer_distribution)

        mode_distribution_offset = np.abs(reference_mode_distribution - evaluation_mode_distribution)
        intermodal_transfer_distribution_offset = np.abs(reference_inter_transfer_distribution - evaluation_inter_transfer_distribution)
        intramodal_transfer_distribution_offset = np.abs(reference_intra_transfer_distribution - evaluation_intra_transfer_distribution)

        return transfers_weight * np.sum(intermodal_transfer_distribution_offset) + transfers_weight * np.sum(intramodal_transfer_distribution_offset) + modes_weight * np.sum(mode_distribution_offset)
    
    else:
         
        reference_transfer_distribution = []
        evaluation_transfer_distribution = []

        for transfers in range(max_transfers + 1):
            f_reference = df_evaluation["transfers_reference"] == transfers
            reference_transfer_distribution.append(df_evaluation.loc[f_reference, "weight"].sum())

            f_evaluation = df_evaluation["transfers_evaluation"] == transfers
            evaluation_transfer_distribution.append(df_evaluation.loc[f_evaluation, "weight"].sum())

        reference_transfer_distribution = np.array(reference_transfer_distribution) / np.sum(reference_transfer_distribution)
        evaluation_transfer_distribution = np.array(evaluation_transfer_distribution) / np.sum(evaluation_transfer_distribution)

        mode_distribution_offset = np.abs(reference_mode_distribution - evaluation_mode_distribution)
        transfer_distribution_offset = np.abs(reference_transfer_distribution - evaluation_transfer_distribution)

        return transfers_weight * np.sum(transfer_distribution_offset) + modes_weight * np.sum(mode_distribution_offset)


# Create the variables
# Using values from the Zurich mode choice model, scaled so that rail TT utility is used as a reference fixed to -1.0
# The tram TT utility is absent from the MCM, using rail TT as a reference
def initialize_variables():
    variables = [
        { "name": "rail_u_h", "initial": -1.0, "fixed": True },
        { "name": "bus_u_h", "initial": -1.72 },
        { "name": "tram_u_h", "initial": -1.0 },
        { "name": "subway_u_h", "copy": "tram_u_h" },
        { "name": "other_u_h", "copy": "bus_u_h" },
        { "name": "wait_u_h", "initial": -1.72},
        { "name": "walk_u_h", "initial": -1.97 },
        { "name": "transfer_u", "initial": 0.0, "fixed": True },
        { "name": "raptorPenalties:transfer_intermodal", "initial": 0.39},
        { "name": "raptorPenalties:transfer_intramodal", "initial": 0.39},
    ]

    # Extend with index information for CMA-ES evaluation
    variables_map = { v["name"]: v for v in variables }

    active_index = 0

    # First find active variables
    for variable in variables:
        if "fixed" in variable or "copy" in variable: 
            continue

        variables_map[variable["name"]] = variable
        
        variable["index"] = active_index
        variable["optimized"] = True
        active_index += 1

    # Then treat fixed variables and those copying from others
    for variable in variables:
        if "fixed" in variable:
            variable["index"] = None
        
        if "copy" in variable:
            assert not "initial" in variable
            variable["initial"] = variables_map[variable["copy"]]["initial"]
            variable["index"] = variables_map[variable["copy"]]["index"]

    return variables


# Prepare function to convert CMA-ES' candidate to utilities
def prepare_utilities(values, variables):
    utilities = {}
    
    for variable in variables:
        if variable["index"] is not None:
            utilities[variable["name"]] = values[variable["index"]]
        else:
            utilities[variable["name"]] = variable["initial"]
    
    return utilities


# Prepare bounds and initial values
def prepare_bounds_and_initial_values(variables):
    initial = []
    bounds = [[], []]

    for variable in variables:
        if "optimized" in variable:
            initial.append(variable["initial"])
            if not "raptorPenalties" in variable["name"]:
                bounds[0].append(-np.inf)
                bounds[1].append(0.0)
            else:
                bounds[0].append(0.0)
                bounds[1].append(np.inf)

    return initial, bounds


if __name__=="__main__":

    parameters               = define_parameters()
    parameters, df_reference = read_reference(parameters)
    requests                 = create_requests(df_reference)
    variables                = initialize_variables()
    initial, bounds          = prepare_bounds_and_initial_values(variables)

    output_path = parameters["output_path"]

    # Choose objective
    objective = parameters["objective"]
    if objective == "observation_based":
        calculate_objective = calculate_objective_observation_based
    elif objective == "distribution_based":
        calculate_objective = calculate_objective_distribution_based
    else:
        raise RuntimeError("Unknown objective")
    
    # Optimizer configuration
    # Configure CMA-ES
    seed       = parameters["seed"]
    sigma      = parameters["sigma"]
    iterations = parameters["iterations"]
    popsize    = parameters["popsize"]

    options = cma.CMAOptions()
    options.set("bounds", bounds)
    options.set("seed", seed)
    options.set("popsize", popsize)

    algorithm = cma.CMAEvolutionStrategy(initial, sigma, options)

    # Load cached data for previous iterations
    history = []

    if os.path.exists(output_path):
        with open(output_path, "rb") as f:
            history = pickle.load(f)

            algorithm.feed_for_resume(
                [h["candidate"] for h in history[1:]], 
                [h["objective"] for h in history[1:]]
        )
            
    # Perform a new batch of iterations
    for iteration in range(iterations):
        initial_evaluation = len(history) == 0
        candidates = [initial]

        if not initial_evaluation:
            candidates = algorithm.ask()

        objectives = []

        # --- Start external process ---
        jar_path    = parameters["jar_path"]

        # --- It is ready so let's optimize ---
        for candidate in candidates:
                utilities   = prepare_utilities(candidate, variables)

                transit_utilities = {}
                for key, value in utilities.items():
                    if not "raptorPenalties" in key:
                        transit_utilities[key] = value

                penalties = []

                for key, value in utilities.items():
                    if "raptorPenalties" in key:
                        if "transfer_intermodal" in key:
                            for themode1 in ["tram", "bus", "rail"]:
                                for themode2 in ["tram", "bus", "rail"]:
                                    if themode1 != themode2:
                                        penalties.append(f"--raptorPenalties:transfer_{themode1}_to_{themode2}")
                                        penalties.append(str(value))
                            
                        if "transfer_intramodal" in key:
                            for themode in ["tram", "bus", "rail"]:
                                penalties.append(f"--raptorPenalties:transfer_{themode}_to_{themode}")
                                penalties.append(str(value))

                server_proc = subprocess.Popen(["java", "-cp", jar_path, "org.eqasim.server.RunServer",
                                         "--config-path", "/home/asallard/Euler_project/inputs/1pct_pt_simulated/switzerland_config_baseline.xml",
                                         "--port", "8054",
                                         "--threads", "12",
                                         "--eqasim-configurator", "org.eqasim.switzerland.ch.SwitzerlandConfigurator",
                                         "--use-transit", "true",
                                         "--config:transitRouter.directWalkFactor","3.0"
                                         ] + penalties)
                try:
                    wait_for_server(parameters["routing_endpoint"], requests[:2], transit_utilities) 

                    df_response = query_endpoint_batched(parameters, requests, transit_utilities)
                    objective   = calculate_objective(parameters, df_reference, df_response)

                    objectives.append(objective)

                    history.append({
                        "candidate": candidate,
                        "utilities": utilities,
                        "objective": objective,
                        "evaluation": df_response,
                        "initial": initial_evaluation
                    })

                finally:
                    server_proc.terminate()
                    try:
                        server_proc.wait(timeout=5)
                    except subprocess.TimeoutExpired:
                        server_proc.kill()

        if not initial_evaluation:
            algorithm.tell(candidates, objectives)
            algorithm.disp()

        # --- Save after a successful CMA-ES iteration ---
        with open(output_path, "wb+") as f:
            pickle.dump(history, f)
