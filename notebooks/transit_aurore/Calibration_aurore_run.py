import pickle, os
import requests as reqlib
from itertools import product

import subprocess
import time

import pandas as pd
import numpy as np
import cma

# TODO create a nice class instead of a dictionary
def define_parameters():

    reference_path   = "/cluster/project/cmdp/asallard/WP4/Route choice/reference_MZ.parquet"
    routing_endpoint = "http://localhost:8054/router/transit"
    output_path      = "/cluster/project/cmdp/asallard/WP4/Route choice/choice-model/results_aurore/transit/calibration.p"
    objective        = "distribution_based"
    jar_path         = "/cluster/project/cmdp/asallard/java/eqasim-java/switzerland/target/switzerland-2.0.0.jar:/cluster/project/cmdp/asallard/java/eqasim-java/server/target/server-2.0.0.jar"
    config_path      = "/cluster/project/cmdp/asallard/inputs/1pct_pt_simulated/switzerland_config_baseline.xml"

    maximum_batch_size = 4000
    modes_weight       = 1.0
    transfers_weight   = 1.0
    eval_intermodality = True

    seed       = 42
    sigma      = 0.5
    iterations = 150
    pop_size   = 6 # Number of candidates generated by each iteration of the evolutionary optimization algorithm

    pt_modes      = ["rail", "subway", "ferry", "tram", "funicular", "cable-car", "gondola", "other", "bus"]
    pt_main_modes = ["rail", "tram", "bus", "pt_other"]

    modes_to_main = {
        "rail": "rail",
        "subway": "tram",
        "ferry": "pt_other",
        "tram": "tram",
        "funicular": "pt_other",
        "cable-car": "pt_other",
        "gondola": "pt_other",
        "other": "pt_other",
        "bus": "bus"
    }

    parameters = {
        "reference_path":         reference_path,
        "routing_endpoint":       routing_endpoint,
        "objective":              objective,
        "output_path":            output_path,
        "maximum_batch_size":     maximum_batch_size,
        "modes_weight":           modes_weight,
        "transfers_weight":       transfers_weight,
        "evaluate_intermodality": eval_intermodality,
        "seed":                   seed,
        "sigma":                  sigma,
        "popsize":                pop_size,
        "iterations":             iterations,
        "jar_path":               jar_path,
        "config_path":            config_path,
        "pt_modes":               pt_modes,
        "pt_main_modes":          pt_main_modes,
        "pt_modes_to_main":       modes_to_main
    }

    return parameters


def is_useful_column(column):
    if column == "trip_id":
        return True
    elif column == "weight":
        return True
    elif "legs" in column or "destination" in column or "origin" in column or "->" in column or "transfers" in column:
        return True
    elif column=="departure_time":
        return True
    return False


# Read the reference file and process it into requests
def read_reference(parameters):

    df_reference = pd.read_parquet(parameters["reference_path"])
    df_reference = df_reference[[col for col in df_reference.columns if is_useful_column(col)]]

    # Identify modes and maximum transfers
    modes             = [c.replace("legs_", "") for c in df_reference.columns if c.startswith("legs_")]
    maximum_transfers = df_reference["transfers"].max()

    ref_modes_to_main = {}
    for mode in modes:
        if mode in parameters["pt_main_modes"]:
            ref_modes_to_main[mode] = mode
        else:
            ref_modes_to_main[mode] = "pt_other"

    parameters["reference_modes"]   = modes
    parameters["max_nb_transfers"]  = maximum_transfers
    parameters["ref_modes_to_main"] = ref_modes_to_main

    all_pairs = [f"{a}->{b}" for a, b in product(modes, repeat=2)]

    rail_to_rail_columns   = [pair for pair in all_pairs if pair == "rail->rail"]
    rail_to_other_columns  = [pair for pair in all_pairs if "rail" in pair and pair != "rail->rail"]
    other_to_other_columns = [pair for pair in all_pairs if "rail" not in pair]

    df_reference["rail_transfers"]       = df_reference[rail_to_rail_columns].sum(axis=1)
    df_reference["other_transfers"]      = df_reference[other_to_other_columns].sum(axis=1)
    df_reference["intermodal_transfers"] = df_reference[rail_to_other_columns].sum(axis=1)

    parameters["max_nb_transfers_intermodal"] = df_reference["intermodal_transfers"].max()
    parameters["max_nb_transfers_rail"]       = df_reference["rail_transfers"].max()
    parameters["max_nb_transfers_other"]      = df_reference["other_transfers"].max()

    df_long             = df_reference.melt(id_vars="trip_id", var_name="mode", value_name="count")
    df_long["mode"]     = df_long["mode"].str.replace("legs_", "")
    df_long["category"] = df_long["mode"].map(ref_modes_to_main)
    df_grouped          = df_long.groupby(["trip_id", "category"], as_index=False)["count"].sum()
    df_result           = df_grouped.pivot(index="trip_id", columns="category", values="count").fillna(0).astype(int)
    df_result           = df_result.add_prefix("legs_").reset_index()

    for column in df_reference.columns:
        if "legs_" in column:
            del df_reference[column]

    initial_length = len(df_reference)
    df_reference   = df_reference.merge(df_result, on = "trip_id", how = "inner")
    final_length   = len(df_reference)

    assert initial_length == final_length

    return parameters, df_reference


def create_requests(df_reference):
    requests = []

    df_reference["request_index"] = np.arange(len(df_reference))

    for _, row in df_reference.iterrows():
        requests.append({
            "request_index": int(row["request_index"]),
            "origin_x": row["origin_x"],
            "origin_y": row["origin_y"],
            "destination_x": row["destination_x"],
            "destination_y": row["destination_y"],
            "departure_time_s": row["departure_time"]
        })

    return requests


def count_transfers(modes):
    intermodal = 0
    rail       = 0
    other      = 0
    for i in range(len(modes) - 1):
        if modes[i] == modes[i + 1]:
            if  modes[i] == "rail":
                rail += 1
            else:
                other += 1
        else:
            if modes[i] == "rail" or modes[i+1] == "rail":
                intermodal += 1
            else:
                other += 1
    return intermodal, rail, other


# Prepare querying the routing server
def query_endpoint(parameters, requests, utilities):
    response = reqlib.post(parameters["routing_endpoint"], json = {
        "batch": requests,
        "utilities": utilities
    })

    assert response.status_code == 200

    df_response = { 
        "request_index": [],
        "transfers": [],
        "rail_transfers": [],
        "other_transfers": [],
        "intermodal_transfers": []
    }

    pt_main_modes    = parameters["pt_main_modes"]
    pt_modes         = parameters["pt_modes"]
    pt_modes_to_main = parameters["pt_modes_to_main"]

    for mode in parameters["pt_main_modes"]:
        df_response["legs_{}".format(mode)] = []

    for row in response.json():
        df_response["request_index"].append(row["request_index"])
        df_response["transfers"].append(row["transfers"])

        count_legs = {}
        
        for mode in pt_modes:
            if mode in row["vehicle_legs_by_mode"]:
                main_mode = pt_modes_to_main[mode]
                if main_mode in count_legs.keys():
                    count_legs[main_mode] += row["vehicle_legs_by_mode"][mode]
                else:
                    count_legs[main_mode] = row["vehicle_legs_by_mode"][mode]

        for main_mode in pt_main_modes:
            if main_mode in count_legs.keys():
                df_response["legs_{}".format(main_mode)].append(count_legs[main_mode])
            else:
                df_response["legs_{}".format(main_mode)].append(0)

        transfers_intermodal, transfers_rail, transfers_other = count_transfers(row["modes_sequence"])
        df_response["intermodal_transfers"].append(transfers_intermodal)
        df_response["rail_transfers"].append(transfers_rail)
        df_response["other_transfers"].append(transfers_other)

    df_response = pd.DataFrame(df_response)

    return df_response


# Send the requests to the server
def query_endpoint_batched(parameters, requests, utilities):
    df_response = []
    batch_index = 0

    maximum_batch_size = parameters["maximum_batch_size"]

    while batch_index * maximum_batch_size < len(requests):
        df_response.append(query_endpoint(parameters, 
            requests[batch_index * maximum_batch_size : (batch_index + 1) * maximum_batch_size],
            utilities))
        
        batch_index += 1
    
    return pd.concat(df_response)


# Set up before the server runs
def wait_for_server(url, requests_sample, utilities, timeout=600, interval=10):
    """Wait until the server at `url` responds successfully."""
    start_time = time.time()
    while time.time() - start_time < timeout:
        print("Waiting for server...")
        try:
            response = reqlib.post(parameters["routing_endpoint"], json = {
                    "batch": requests_sample,
                    "utilities": utilities
                })
            if response.status_code == 200:
                print("The server is running!")
                return True
        except reqlib.ConnectionError:
            pass
        time.sleep(interval)
    raise TimeoutError(f"Server at {url} didn't start within {timeout} seconds.")


# Define the optimization objective: observation comparison based
def calculate_objective_observation_based(parameters, df_reference, df_evaluation):
    transfers_weight = parameters["transfers_weight"]
    modes_weight     = parameters["modes_weight"]
    modes            = parameters["pt_main_modes"]

    df_evaluation = pd.merge(df_reference, df_evaluation, on = "request_index", 
        suffixes = ["_reference", "_evaluation"])
    
    df_evaluation["offset"] = transfers_weight * np.abs(
        df_evaluation["transfers_reference"] - df_evaluation["transfers_evaluation"])

    for mode in modes:
        df_evaluation["offset"] += modes_weight * np.abs(
            df_evaluation["legs_{}_reference".format(mode)] - df_evaluation["legs_{}_evaluation".format(mode)]
        )

    return np.sum(df_evaluation["offset"] * df_evaluation["weight"]) / df_evaluation["weight"].sum()


# Define the optimization objective: distribution comparison based
def calculate_objective_distribution_based(parameters, df_reference, df_evaluation):
    transfers_weight = parameters["transfers_weight"]
    modes_weight     = parameters["modes_weight"]
    modes            = parameters["pt_main_modes"]
    max_transfers    = parameters["max_nb_transfers"] 

    df_evaluation = pd.merge(df_reference, df_evaluation, on = "request_index", 
        suffixes = ["_reference", "_evaluation"])
    
    reference_mode_distribution = []
    evaluation_mode_distribution = []

    for mode in modes:
        reference_mode_distribution.append((df_evaluation["legs_{}_reference".format(mode)] * df_evaluation["weight"]).sum())
        evaluation_mode_distribution.append((df_evaluation["legs_{}_evaluation".format(mode)] * df_evaluation["weight"]).sum())

    reference_mode_distribution = np.array(reference_mode_distribution) / np.sum(reference_mode_distribution)
    evaluation_mode_distribution = np.array(evaluation_mode_distribution) / np.sum(evaluation_mode_distribution)

    if parameters["evaluate_intermodality"]:

        max_intermodal_transfers = parameters["max_nb_transfers_intermodal"]
        max_rail_transfers       = parameters["max_nb_transfers_rail"]
        max_other_transfers      = parameters["max_nb_transfers_other"]

        max_nb = {"intermodal": max_intermodal_transfers,
                  "rail":       max_rail_transfers,
                  "other":      max_other_transfers}
        
        transfers_offsets = {}

        for transfer_category in ["intermodal", "rail", "other"]:

            reference_transfers  = []
            evaluation_transfers = []

            for transfers in range(max_nb[transfer_category] + 1):
                f_reference = df_evaluation[transfer_category + "_transfers_reference"] == transfers
                reference_transfers.append(df_evaluation.loc[f_reference, "weight"].sum())

                f_evaluation = df_evaluation[transfer_category + "_transfers_evaluation"] == transfers
                evaluation_transfers.append(df_evaluation.loc[f_evaluation, "weight"].sum())

            reference_transfers  = np.array(reference_transfers) / np.sum(reference_transfers)
            evaluation_transfers =  np.array(evaluation_transfers) / np.sum(evaluation_transfers)

            transfers_offsets[transfer_category] = np.abs(reference_transfers - evaluation_transfers)

        transfer_distribution_offset = 0
        for _, offset in transfers_offsets.items():
            transfer_distribution_offset += np.sum(offset)

        mode_distribution_offset = np.abs(reference_mode_distribution - evaluation_mode_distribution)

        return transfers_weight * transfer_distribution_offset + modes_weight * np.sum(mode_distribution_offset)
    
    else:
         
        reference_transfer_distribution = []
        evaluation_transfer_distribution = []

        for transfers in range(max_transfers + 1):
            f_reference = df_evaluation["transfers_reference"] == transfers
            reference_transfer_distribution.append(df_evaluation.loc[f_reference, "weight"].sum())

            f_evaluation = df_evaluation["transfers_evaluation"] == transfers
            evaluation_transfer_distribution.append(df_evaluation.loc[f_evaluation, "weight"].sum())

        reference_transfer_distribution = np.array(reference_transfer_distribution) / np.sum(reference_transfer_distribution)
        evaluation_transfer_distribution = np.array(evaluation_transfer_distribution) / np.sum(evaluation_transfer_distribution)

        mode_distribution_offset = np.abs(reference_mode_distribution - evaluation_mode_distribution)
        transfer_distribution_offset = np.abs(reference_transfer_distribution - evaluation_transfer_distribution)

        return transfers_weight * np.sum(transfer_distribution_offset) + modes_weight * np.sum(mode_distribution_offset)


# Create the variables
# Using values from the Zurich mode choice model, scaled so that rail TT utility is used as a reference fixed to -1.0
# The tram TT utility is absent from the MCM, using rail TT as a reference
def initialize_variables():
    variables = [
        { "name": "rail_u_h", "initial": -1.0, "fixed": True },
        { "name": "bus_u_h", "initial": -1.72 },
        { "name": "tram_u_h", "initial": -1.0 },
        { "name": "subway_u_h", "copy": "tram_u_h" },
        { "name": "other_u_h", "copy": "bus_u_h" },
        { "name": "wait_u_h", "initial": -1.72},
        { "name": "walk_u_h", "initial": -1.97 },
        { "name": "transfer_u", "initial": 0.0, "fixed": True },
        { "name": "raptorPenalties:transfer_intermodal", "initial": 0.39},
        { "name": "raptorPenalties:transfer_rail", "initial": 0.39},
        { "name": "raptorPenalties:transfer_other", "initial": 0.39},
    ]

    # Extend with index information for CMA-ES evaluation
    variables_map = { v["name"]: v for v in variables }

    active_index = 0

    # First find active variables
    for variable in variables:
        if "fixed" in variable or "copy" in variable: 
            continue

        variables_map[variable["name"]] = variable
        
        variable["index"] = active_index
        variable["optimized"] = True
        active_index += 1

    # Then treat fixed variables and those copying from others
    for variable in variables:
        if "fixed" in variable:
            variable["index"] = None
        
        if "copy" in variable:
            assert not "initial" in variable
            variable["initial"] = variables_map[variable["copy"]]["initial"]
            variable["index"] = variables_map[variable["copy"]]["index"]

    return variables


# Prepare function to convert CMA-ES' candidate to utilities
def prepare_utilities(values, variables):
    utilities = {}
    
    for variable in variables:
        if variable["index"] is not None:
            utilities[variable["name"]] = values[variable["index"]]
        else:
            utilities[variable["name"]] = variable["initial"]
    
    return utilities


# Prepare bounds and initial values
def prepare_bounds_and_initial_values(variables):
    initial = []
    bounds = [[], []]

    for variable in variables:
        if "optimized" in variable:
            initial.append(variable["initial"])
            if not "raptorPenalties" in variable["name"]:
                bounds[0].append(-np.inf)
                bounds[1].append(0.0)
            else:
                bounds[0].append(0.0)
                bounds[1].append(np.inf)

    return initial, bounds


if __name__=="__main__":

    parameters               = define_parameters()
    parameters, df_reference = read_reference(parameters)
    requests                 = create_requests(df_reference)
    variables                = initialize_variables()
    initial, bounds          = prepare_bounds_and_initial_values(variables)

    output_path = parameters["output_path"]

    # Choose objective
    objective = parameters["objective"]
    if objective == "observation_based":
        calculate_objective = calculate_objective_observation_based
    elif objective == "distribution_based":
        calculate_objective = calculate_objective_distribution_based
    else:
        raise RuntimeError("Unknown objective")
    
    # Optimizer configuration
    # Configure CMA-ES
    seed       = parameters["seed"]
    sigma      = parameters["sigma"]
    iterations = parameters["iterations"]
    popsize    = parameters["popsize"]

    options = cma.CMAOptions()
    options.set("bounds", bounds)
    options.set("seed", seed)
    options.set("popsize", popsize)

    algorithm = cma.CMAEvolutionStrategy(initial, sigma, options)

    pt_modes = parameters["pt_modes"]

    # Load cached data for previous iterations
    history = []

    if os.path.exists(output_path):
        with open(output_path, "rb") as f:
            history = pickle.load(f)

            algorithm.feed_for_resume(
                [h["candidate"] for h in history[1:]], 
                [h["objective"] for h in history[1:]]
        )
            
    # Perform a new batch of iterations
    for iteration in range(iterations):
        initial_evaluation = len(history) == 0
        candidates = [initial]

        if not initial_evaluation:
            candidates = algorithm.ask()

        objectives = []

        # --- Start external process ---
        jar_path    = parameters["jar_path"]

        # --- It is ready so let's optimize ---
        for candidate in candidates:
                utilities   = prepare_utilities(candidate, variables)

                transit_utilities = {}
                for key, value in utilities.items():
                    if not "raptorPenalties" in key:
                        transit_utilities[key] = value

                penalties = []

                for key, value in utilities.items():
                    if "raptorPenalties" in key:
                        if "transfer_intermodal" in key:
                            for themode1 in pt_modes:
                                for themode2 in pt_modes:
                                    if themode1 != themode2 and "rail" in [themode1, themode2]:
                                        penalties.append(f"--raptorPenalties:transfer_{themode1}_to_{themode2}")
                                        penalties.append(str(value))
                            
                        if "transfer_rail" in key:
                            themode = "rail"
                            penalties.append(f"--raptorPenalties:transfer_{themode}_to_{themode}")
                            penalties.append(str(value))

                        if "transfer_other" in key:
                            for themode1 in pt_modes:
                                for themode2 in pt_modes:
                                    if not ("rail" in [themode1, themode2]):
                                        penalties.append(f"--raptorPenalties:transfer_{themode1}_to_{themode2}")
                                        penalties.append(str(value))

                server_proc = subprocess.Popen(["java", "-cp", jar_path, "org.eqasim.server.RunServer",
                                         "--config-path", parameters["config_path"],
                                         "--port", "8054",
                                         "--threads", "12",
                                         "--eqasim-configurator", "org.eqasim.switzerland.ch.SwitzerlandConfigurator",
                                         "--use-transit", "true",
                                         "--config:transitRouter.directWalkFactor","3.0"
                                         ] + penalties)
                try:
                    wait_for_server(parameters["routing_endpoint"], requests[:2], transit_utilities) 

                    df_response = query_endpoint_batched(parameters, requests, transit_utilities)
                    objective   = calculate_objective(parameters, df_reference, df_response)

                    objectives.append(objective)

                    history.append({
                        "candidate": candidate,
                        "utilities": utilities,
                        "objective": objective,
                        "evaluation": df_response,
                        "initial": initial_evaluation
                    })

                finally:
                    server_proc.terminate()
                    try:
                        server_proc.wait(timeout=5)
                    except subprocess.TimeoutExpired:
                        server_proc.kill()

        if not initial_evaluation:
            algorithm.tell(candidates, objectives)
            algorithm.disp()

        # --- Save after a successful CMA-ES iteration ---
        with open(output_path, "wb+") as f:
            pickle.dump(history, f)
